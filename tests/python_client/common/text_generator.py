from faker import Faker
import random

class ICUTextGenerator:
    """
    ICU(International Components for Unicode)TextGenerator: 
    Generate test sentences containing multiple languages (Chinese, English, Japanese, Korean), emojis, and special symbols.
    """
    def __init__(self):
        self.fake_en = Faker("en_US")
        self.fake_zh = Faker("zh_CN")
        self.fake_ja = Faker("ja_JP")
        self.fake_de = Faker("de_DE")
        self.korean_samples = [
            "ì•ˆë…•í•˜ì„¸ìš” ì„¸ê³„", "íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë°", "ë°ì´í„° ë¶„ì„", "ì¸ê³µì§€ëŠ¥",
            "ë°€ë²„ìŠ¤ í…ŒìŠ¤íŠ¸", "í•œêµ­ì–´ ìƒ˜í”Œ", "ìì—°ì–´ ì²˜ë¦¬"
        ]
        self.emojis = ["ğŸ˜Š", "ğŸ", "ğŸš€", "ğŸŒ", "ğŸ’¡", "ğŸ”¥", "âœ¨", "ğŸ‘"]
        self.specials = ["#", "@", "$"]

    def word(self):
        """
        Generate a list of words containing multiple languages, emojis, and special symbols.
        """
        parts = [
            self.fake_en.word(),
            self.fake_zh.word(),
            self.fake_ja.word(),
            self.fake_de.word(),
            random.choice(self.korean_samples),
            random.choice(self.emojis),
            random.choice(self.specials),
        ]
        return  random.choice(parts)

    def sentence(self):
        """
        Generate a sentence containing multiple languages, emojis, and special symbols.
        """
        parts = [
            self.fake_en.sentence(),
            self.fake_zh.sentence(),
            self.fake_ja.sentence(),
            self.fake_de.sentence(),
            random.choice(self.korean_samples),
            " ".join(random.sample(self.emojis, 2)),
            " ".join(random.sample(self.specials, 2)),
        ]
        random.shuffle(parts)
        return " ".join(parts)

    def paragraph(self, num_sentences=3):
        """
        Generate a paragraph containing multiple sentences, each with multiple languages, emojis, and special symbols.
        """
        return ' '.join([self.sentence() for _ in range(num_sentences)])

    def text(self, num_sentences=5):
        """
        Generate multiple sentences containing multiple languages, emojis, and special symbols.
        """
        return ' '.join([self.sentence() for _ in range(num_sentences)])


class KoreanTextGenerator:
    """
    KoreanTextGenerator: Generate test sentences containing Korean activities, verbs, connectors, and modifiers.
    """
    def __init__(self):
        # Sports/Activities (Nouns)
        self.activities = [
            "ìˆ˜ì˜", "ì¶•êµ¬", "ë†êµ¬", "í…Œë‹ˆìŠ¤",
            "ë°°êµ¬", "ì•¼êµ¬", "ê³¨í”„", "ëŸ­ë¹„",
            "ë‹¬ë¦¬ê¸°", "ìì „ê±°", "ìŠ¤ì¼€ì´íŠ¸", "ìŠ¤í‚¤",
            "ì„œí•‘", "ë‹¤ì´ë¹™", "ë“±ì‚°", "ìš”ê°€",
            "ì¶¤", "í•˜ì´í‚¹", "ë…ì„œ", "ìš”ë¦¬"
        ]

        # Verbs (Base Form)
        self.verbs = [
            "ì¢‹ì•„í•˜ë‹¤", "ì¦ê¸°ë‹¤", "í•˜ë‹¤", "ë°°ìš°ë‹¤",
            "ê°€ë¥´ì¹˜ë‹¤", "ë³´ë‹¤", "ì‹œì‘í•˜ë‹¤", "ê³„ì†í•˜ë‹¤",
            "ì—°ìŠµí•˜ë‹¤", "ì„ í˜¸í•˜ë‹¤", "ë§ˆìŠ¤í„°í•˜ë‹¤", "ë„ì „í•˜ë‹¤"
        ]

        # Connectors
        self.connectors = [
            "ê·¸ë¦¬ê³ ", "ë˜ëŠ”", "í•˜ì§€ë§Œ", "ê·¸ëŸ°ë°",
            "ê·¸ë˜ì„œ", "ë˜í•œ", "ê²Œë‹¤ê°€", "ê·¸ëŸ¬ë©´ì„œ",
            "ë™ì‹œì—", "í•¨ê»˜"
        ]

        # Modifiers (Frequency/Degree)
        self.modifiers = [
            "ë§¤ìš°", "ìì£¼", "ê°€ë”", "ì—´ì‹¬íˆ",
            "ì „ë¬¸ì ìœ¼ë¡œ", "ê·œì¹™ì ìœ¼ë¡œ", "ë§¤ì¼", "ì¼ì£¼ì¼ì— í•œ ë²ˆ",
            "ì·¨ë¯¸ë¡œ", "ì§„ì§€í•˜ê²Œ"
        ]

    def conjugate_verb(self, verb):
        # Simple Korean verb conjugation (using informal style "-ì•„/ì–´ìš”")
        if verb.endswith("í•˜ë‹¤"):
            return verb.replace("í•˜ë‹¤", "í•´ìš”")
        elif verb.endswith("ë‹¤"):
            return verb[:-1] + "ì•„ìš”"
        return verb


    def word(self):
        return random.choice(self.activities + self.verbs + self.modifiers + self.connectors)

    def sentence(self):
        # Build basic sentence structure
        activity = random.choice(self.activities)
        verb = random.choice(self.verbs)
        modifier = random.choice(self.modifiers)

        # Conjugate verb
        conjugated_verb = self.conjugate_verb(verb)

        # Build sentence (Korean word order: Subject + Object + Modifier + Verb)
        sentence = f"ì €ëŠ” {activity}ë¥¼/ì„ {modifier} {conjugated_verb}"

        # Randomly add connector and another activity
        if random.choice([True, False]):
            connector = random.choice(self.connectors)
            second_activity = random.choice(self.activities)
            second_verb = self.conjugate_verb(random.choice(self.verbs))
            sentence += f" {connector} {second_activity}ë„ {second_verb}"

        return sentence + "."

    def paragraph(self, num_sentences=3):
        return '\n'.join([self.sentence() for _ in range(num_sentences)])

    def text(self, num_sentences=5):
        return '\n'.join([self.sentence() for _ in range(num_sentences)])


def generate_text_by_analyzer(analyzer_params):
    """
    Generate text data based on the given analyzer parameters

    Args:
        analyzer_params: Dictionary containing the analyzer parameters

    Returns:
        str: Generated text data
    """
    if analyzer_params["tokenizer"] == "standard":
        fake = Faker("en_US")
    elif analyzer_params["tokenizer"] == "jieba":
        fake = Faker("zh_CN")
    elif analyzer_params["tokenizer"] == "icu":
        fake = ICUTextGenerator()

    elif analyzer_params["tokenizer"]["type"] == "lindera":
        # Generate random Japanese text
        if analyzer_params["tokenizer"]["dict_kind"] == "ipadic":
            fake = Faker("ja_JP")
        elif analyzer_params["tokenizer"]["dict_kind"] == "ko-dic":
            fake = KoreanTextGenerator()
        elif analyzer_params["tokenizer"]["dict_kind"] == "cc-cedict":
            fake = Faker("zh_CN")
        else:
            raise ValueError("Invalid dict_kind")
    else:
        raise ValueError("Invalid analyzer parameters")

    text = fake.text()
    stop_words = []
    if "filter" in analyzer_params:
        for filter in analyzer_params["filter"]:
            if filter["type"] == "stop":
                stop_words.extend(filter["stop_words"])

    # add stop words to the text
    text += " " + " ".join(stop_words)
    return text
